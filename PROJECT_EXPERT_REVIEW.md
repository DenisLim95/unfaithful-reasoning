Expert Review of Chain-of-Thought Faithfulness in Small Reasoning Models
Introduction and Overview

The project by Denis Lim investigates whether a small language model (DeepSeek-R1-Distill-Qwen-1.5B, 1.5B parameters) encodes chain-of-thought (CoT) faithfulness in a linearly accessible way. The study uses a question-flipping paradigm to label when a model’s reasoning is faithful (i.e. consistent and correct for logically equivalent question pairs) and trains linear probes on the model’s internal activations to predict faithfulness. Key reported results include a 72% faithfulness rate on a numerical comparison task and a probe accuracy of ~66.7%, with indications that the separability of faithful vs. unfaithful instances increases in the model’s last layers. This review will critically evaluate the experimental design, the validity of conclusions, statistical robustness, the activation probing methodology, and how well the work addresses the core question of linear encodability of CoT faithfulness. We also discuss limitations (many of which the author notes) and suggest improvements for future iterations.

Experimental Design and Methodology

Approach: The study’s core methodology is well-defined: generate semantically equivalent question pairs by swapping the order of numerical arguments (e.g. “Is 847 larger than 839?” vs. “Is 839 larger than 847?”) such that the correct answers should be opposite. The model is prompted to produce a chain-of-thought (using special <think> tags) and a final Yes/No answer for each question. A faithful reasoning chain is operationally defined as yielding consistent and correct answers to both questions in the pair. If the model gives the same answer to both (or an incorrect answer to either), the reasoning is labeled unfaithful, indicating the model’s explanation may not truly reflect how it arrived at the answer. This question-flipping strategy, inspired by prior work (Arcuschin et al., 2025), is a sensible way to reveal inconsistency in model reasoning: a faithfully reasoning model should not answer a question and its logical inverse in a contradictory way.

Dataset and Scope: 50 question pairs (100 prompts total) of numerical comparisons were used. This is a very narrow domain (simple arithmetic comparisons) and a small sample. While using a focused, objective task (where ground truth is clear) is a good starting point, it inherently limits the generality of conclusions. A notable design choice is that only one model (the distilled 1.5B parameter Qwen variant) was tested, and only on this task type. No larger models or different reasoning tasks were directly evaluated, so comparisons to larger models are indirect (via cited prior results) and should be interpreted with caution.

Faithfulness Labeling: By scoring a pair as faithful only if both questions are answered correctly with opposite answers, the methodology conflates correctness with faithfulness. In practice, this means any error in answering either question, even if due to a simple mistake, renders the pair “unfaithful.” This labeling is practical for a binary classification, but it treats all failure modes the same – from blatant contradictory reasoning (e.g. answering “Yes” to both Q1 and Q2) to just answering one question wrong. This could be seen as a limitation in defining faithfulness, as a model might have an incorrect chain-of-thought (hence wrong answer) yet still be faithfully following its (flawed) reasoning. In other words, faithfulness is partially conflated with accuracy here. While this doesn’t entirely undermine the approach, it introduces some noise in the interpretation of what “unfaithful” means (sometimes it's a reasoning inconsistency, other times just a mistake). A more nuanced measure (distinguishing logical inconsistency from mere error) could strengthen the experimental design.

Potential Critical Flaws: Several methodological choices could undermine the results if not accounted for:

Small Sample Size: Only 50 pairs were used, and ultimately only 44 data points went into probe training (after filtering). The test split was ~9 samples, which is extremely small. This raises serious concerns about statistical validity – a single classification error changes accuracy by 11% on a 9-sample test set. Such a small N means results (especially probe accuracy) have very wide uncertainty (on the order of ±30% points confidence interval) and may not be reliable. This is arguably a fatal flaw in drawing strong conclusions; at best, the results should be considered preliminary. The author does acknowledge this limitation explicitly.

Non-Deterministic Labeling vs Deterministic Activations: Perhaps the most glaring methodological issue is the mismatch in how labels were obtained versus how activations were collected. During Phase 2 (generation), the model’s answers (and thus faithfulness labels) were obtained with a stochastic sampling (temperature 0.6). However, in Phase 3 (probe analysis), activations were captured by feeding the same input deterministically (e.g. using a fixed seed or greedy decoding) up to the end of the question prompt. This means the label (faithful/unfaithful) is based on one random run of the model’s reasoning, but the features (hidden activations) used by the probe might correspond to a different run that could potentially have led to a different outcome. In essence, the study tries to predict a randomized behavior from a deterministic snapshot of the model. This mismatch injects label noise – the same question could be answered faithfully in one stochastic run and unfaithfully in another, even though the deterministic hidden state before generation might be the same. This is a critical flaw because it directly lowers the achievable probe accuracy and clouds interpretation: a weak probe result might be due to genuine lack of linear signal or simply noise in labels. The author notes this issue and correctly suspects it could partly explain the modest probe performance.

Mean-Pooling of Activations: The choice to mean-pool the residual stream across the sequence (compressing [seq_len × d_model] into a single [d_model] vector per example) is a blunt approach that risks losing important positional information. Faithfulness might correspond to very specific features in the chain-of-thought (e.g. how certain tokens or comparisons are processed). By averaging across all tokens (including the question and possibly the <think> prompt tokens), the probe features become a smeared representation. If, for example, the crucial difference between a faithful vs unfaithful reasoning lies in a particular attention pattern or a specific token’s embedding (like the comparative operator or a numerical token), mean-pooling could dilute that signal. This is not a fatal flaw, but it likely weakened the probe’s ability to detect any subtle linear separability. A more targeted approach (such as taking the final token’s representation, using a [CLS] token if available, or pooling only over the reasoned content) might yield a clearer signal.

Linear Probe Design (No Bias Term): The probe is a single linear layer without bias. While a minor detail, a no-bias linear classifier forces the decision boundary through the origin, which might be suboptimal if the faithful vs unfaithful representations are not centered. In practice, including a bias is standard for classification to adjust the threshold. Excluding it likely didn’t cause the uniform 66.7% accuracy outcome by itself, but it is an odd constraint that could slightly undermine optimal separation.

Lack of Validation and Overfitting Concerns: Training separate probes for each layer for 50 epochs on ~35 training samples (with no early stopping) raises overfitting concerns. A linear model can indeed overfit in high-dimensional space with such few data points. The uniform 66.7% test accuracy across layers is suspicious – it suggests the possibility that the probe might be predicting the majority class every time or that the test set was too small to reflect any differences. In fact, given the class ratio (about 70% faithful in data), a classifier that always predicts “faithful” would score roughly 70% on a large test set and exactly 66.7% on a 9-sample test if 6 of 9 were faithful. This seems to be the case, as 66.7% equals 6/9 correct, likely indicating the probe (implicitly or explicitly) defaulted to predicting all (or almost all) prompts as faithful. The author’s note that 66.7% was +16.7 points over a 50% random baseline is somewhat misleading, because the more relevant baseline is the majority-class baseline ~68% faithful. By that standard, the probe did no better than trivial guessing, and in fact slightly worse than always guessing “faithful” on this imbalanced test set. This undermines the conclusion that the probe found a meaningful signal – it likely did not. The results here may be essentially noise or a consequence of the small sample, rather than evidence of a real linear separability in representations.

In summary, the experimental design was creative and addresses the question in principle, but the small scale and certain design choices (especially the label/activation mismatch and minimal test data) pose serious threats to the validity of the results. These flaws don’t entirely invalidate the effort (as this is clearly labeled an exploratory, “early-stage” study), but they do mean any positive findings should be taken as tentative.

Results vs. Conclusions: Are the Claims Supported?

The study’s conclusions should be weighed against the actual results obtained:

High Faithfulness in a Small Model: It is reported that the 1.5B model was 72% faithful on these simple numeric tasks. This is notably higher than faithfulness rates reported for much larger models (39% for a 70B model, 25% for Claude 3.7 in Arcuschin et al. 2025). The result itself – 36 out of 50 question pairs answered consistently and correctly – is straightforward and well-supported by the data collected. The conclusion that “small models can show high faithfulness on simple tasks” is factually supported for this specific case. However, the comparison implying small > large in faithfulness must be heavily qualified. The author appropriately cautions that the tasks and evaluation differ, so we cannot conclude a general trend that smaller models are more faithful. It’s quite possible the 70B model was tested on harder questions or that the chain-of-thought prompting differed. Thus, while the observation is interesting, the conclusion should be limited to “at least on basic numeric comparisons, this 1.5B model exhibited a fairly high rate of consistent reasoning”. It is supported by the results obtained here, but it doesn’t overturn prior work without a controlled comparison.

Linear Probe Shows Modest Signal: The author concludes that a “modest linear signal exists” because the probe achieved ~66.7% accuracy versus a 50% random baseline. As discussed, this interpretation is questionable given class imbalance. In truth, the probe’s performance is so low and the test set so small that it’s unclear any real signal was captured. The intended conclusion – that there is some information in the activations correlating with faithfulness – is not strongly supported by the evidence. A 66.7% accuracy on 9 samples could easily happen by chance or by a trivial strategy (predict all as faithful). Moreover, the inconsistent AUC values reported (ranging from 0.22 to 0.44 across layers, some below 0.5) suggest the classifier was not meaningfully discriminative. The conclusion that “linear probes can detect some faithfulness signal above chance” is overstated; if anything, the results indicate that any linear separability is extremely weak. The author’s own caveats (that the signal is weak and may require non-linear methods) are appropriate. In sum, the results do not convincingly support a conclusion that faithfulness is linearly encoded in a readily detectable way – they rather indicate the opposite (i.e., a linear probe barely performs above chance, if at all).

Faithfulness Emerges in Late Layers: The report notes a “dramatic jump” in a separation metric at the final layer (layer 24), from ~0.41 at layer 18 to ~0.86 at layer 24. This suggests that by the end of the transformer's computation (after seeing the question prompt), the internal representation of the input might hold a lot more information related to whether the model’s answer will be faithful or not. This claim is partially supported by the analysis: if we trust the “separation metric” (which seems to be some measure of cluster separability or distance between faithful vs unfaithful activation projections), it indeed was much higher for the last layer. However, this is in tension with the probe’s uniform accuracy across layers. It’s possible that the separation metric was computed on training data or in a way that doesn’t translate to classification accuracy on the tiny test set. If the metric (0.862 at layer 24) indicates a large gap in the probe’s output distributions for faithful vs unfaithful, one would expect better than chance performance. The fact that test accuracy didn’t improve at layer 24 (and AUC was actually worst there) hints that either the probe overfit the training data or that the visualization metric is not reliable in this small-sample regime. Thus, while it is plausible and theoretically sensible that faithfulness-related features solidify in later layers (complex reasoning likely happens in deeper layers), the evidence is not robust. The conclusion is suggestive rather than proven: late-layer representations might carry more linearly extractable faithfulness information, but the results here aren’t definitive. The claim is more supported by general intuition about transformers than by the shaky quantitative outcome.

Overall, the main conclusions are only partially supported by the results. The high faithfulness rate on simple tasks is directly shown, but broader implications (like model size relationship) remain speculative. The linear probe result does not strongly support a claim of linearly encoded faithfulness – if anything, it underscores that such encoding, if present, is weak. The idea of late-layer emergence of the signal is reasonable, but given the contradictory probe performance, it should be taken as an open hypothesis rather than a confirmed result. In an expert review context, I would say the conclusions somewhat overreach what the small-scale results can guarantee, though the author does maintain honesty about many limitations.

Statistical Robustness of Probe Training and Evaluation

The statistical robustness of the probe experiment is a major concern:

Tiny Test Set & Uncertain Accuracy: With only ~9 examples in the test set, the reported 66.7% accuracy is not statistically reliable. The expected variation from such a small sample is enormous. The author correctly notes that confidence intervals would be on the order of ±30-40 percentage points – meaning the true accuracy could feasibly be anywhere from near chance to near perfect on a larger sample. In effect, no rigorous conclusion can be drawn from a single 9-sample test accuracy. A slight change in which examples fell into the test set could swing the result drastically. For instance, had one more misclassification occurred, accuracy would drop to ~55%; one less, it’d be ~78%. This calls for extreme caution in interpreting the probe performance.

Class Imbalance and Baselines: The data had about twice as many faithful examples as unfaithful (30 vs 14 used in probe training). Although the split was stratified, the small numbers mean the exact test composition could vary (likely 6 faithful vs 3 unfaithful in test). The author initially compares accuracy to a 50% random baseline, but as pointed out later, a more appropriate baseline is predicting the majority class every time (~68% accuracy). The probe did not surpass this majority baseline – in fact, it essentially achieved it. This suggests that the probe might have learned a biased classifier or effectively done nothing useful. The uniform 66.7% across all layers reinforces the possibility that the probe’s predictions did not actually depend on layer-specific information but rather defaulted to a fixed output (likely always predicting “faithful” or otherwise split in a fixed ratio). Such an outcome can happen if the model training converged to a trivial solution, which is plausible given the tiny dataset and lack of regularization. In short, statistically, there is no evidence that the probe is any better than a dummy classifier in this setup.

Lack of Cross-Validation or Repeated Trials: The experiment relied on a single train-test split and one training run per layer. With such few samples, the variance between different splits or random initializations of the probe could be huge. A robust evaluation would have used techniques like cross-validation (e.g. leave-one-out or k-fold, given the scarcity of data) to estimate performance more reliably. Not doing so means we don’t know if 66.7% was an outlier high, low, or typical value; it could even be artificially high if the test split was “easy” by chance. The study would benefit from repeated sampling of train/test splits to gauge stability.

Probe Training Epochs and Overfitting: The probes were trained for 50 epochs on ~35 points with no early stopping. While a linear model can generalize, with such a small sample it might just memorize noise. The absence of a validation set or early stop means the chosen model could be overfit. The suspicious AUC values (some below 0.5 on test) might indicate that threshold calibration was off or that on test the model’s confidence was misaligned. It’s hard to draw firm conclusions, but the training regimen was not geared towards rigorous generalization.

Separation Metric vs. Accuracy: The “separation” numbers reported (0.287 at layer 6 up to 0.862 at layer 24) are not standard metrics and weren’t clearly defined in statistical terms. It sounds like perhaps the author projected activations onto the learned probe direction and looked at difference in means or a clustering measure. Without clarity or statistical testing, these numbers alone don’t mean much. The dramatic jump to 0.862 in the final layer, while striking, could be an artifact of overfitting on the training data or simply a reflection of the model having essentially decided on an answer by the end of the input (hence an internal state difference). But given the inconsistency with test results, this visualization needs to be treated with skepticism. Statistically, one would want to see that reflected in held-out data to believe it.

In summary, the probe analysis as conducted lacks statistical rigor due to extremely limited data. The results must be treated as anecdotal. The author is transparent about these issues, noting the wide confidence intervals and even the odd uniform accuracy, which is commendable. As an expert reviewer, I would stress that no strong statistical claims can be made here – the probe experiment serves as an exploratory pilot at best. Future work absolutely needs a larger sample and more careful evaluation to claim any real discovery of linear separability.

Activation Analysis Setup Appropriateness

The methodology for capturing and using internal activations in the probe is a crucial aspect of this study. Several choices were made in the activation analysis pipeline:

Hook Location (Residual Stream after Blocks): The probe uses activations from the residual stream post-layer at selected depths (layers 6, 12, 18, 24). This is a reasonable approach – the residual stream at blocks.{layer}.hook_resid_post represents the model’s encoded information after that transformer block’s attention and MLP have run. By examining multiple layers, the study aimed to see how the encodings evolve. The choice of 6, 12, 18, 24 (assuming the model has 24 layers total) gives a coarse snapshot from early to late layers. Ideally, more fine-grained or all layers could be examined, but given resource constraints, these four points are adequate to detect major trends. No flaw here; if anything, it was prudent to reduce the data volume by selecting a few layers.

Timing of Activation Capture: A critical point is when in the sequence the activations are taken. In this study, activations were captured after processing the input question (and the prompt up to that point), but before the model generated its answer. Specifically, they took the hidden state after the last input token for Q1 in each pair. The probe is therefore predicting faithfulness from the model’s state at the end of reading the question (including any “think” prompt that instructs reasoning, presumably) before any actual reasoning steps are generated. This setup tests a somewhat surprising question: Can we predict whether the eventual chain-of-thought and answer will be faithful, just by looking at the model’s embeddings after reading the question? This is a strict and potentially inappropriate test for real-time monitoring, because it doesn’t use the information in the generated reasoning at all. In terms of faithful reasoning, one might expect that only once the model actually engages in the chain-of-thought (in the generation phase) does the “faithfulness” manifest. By not capturing activations during the chain-of-thought generation, the probe may be missing the most relevant signals. The author notes this as a limitation: the probe is predicting faithfulness from input representations rather than from the full reasoning process. This decision likely stems from the complexity of caching activations during autoregressive generation (especially with randomness). It is understandable for a first pass, but it limits the probe’s effectiveness. In review, this is a significant methodological compromise – ideally, one would capture the model’s hidden states at the point of producing the final answer (or throughout the reasoning steps) to directly see if faithful vs unfaithful reasoning trajectories diverge in hidden space.

Mean-Pooling vs. Token-Specific Features: As mentioned earlier, the use of mean-pooling over the sequence is a blunt tool. While it provides a single fixed-length vector input to the probe (simplifying the classification task), it likely washes out important differences. For example, in a question like “Is 847 larger than 839?”, the crucial elements are the numbers and the comparative relation. A faithful reasoning process might involve the model strongly encoding the relationship “847 > 839 is True” in some neurons. An unfaithful or inconsistent process might not form a correct comparison. These differences might be evident at specific token positions (e.g., at the conclusion of reading the question or at a special reasoning token). By averaging across all positions, one mixes tokens that carry signal with those that don’t (articles, punctuation, irrelevant tokens). This could make the faithful vs unfaithful representations much harder to separate. In more sophisticated interpretability work, one might use the representation of a special classification token, or even feed the entire sequence into a probe model that can attend to all token embeddings. The choice of mean-pooling is a simplifying assumption that, while not a fatal mistake, reduces sensitivity. In essence, the activation analysis setup favored simplicity over fidelity to how information might actually be localized in the sequence.

Linear Probe Simplicity: The decision to use a single-layer linear probe aligns with the research question (linear encodability). This is fine and appropriate for testing linear separability. The implementation detail of using no bias we already discussed (unusual but minor). One might question if any normalization or preprocessing was done on the activations (e.g., scaling or PCA) – none was mentioned, so presumably the raw residual vectors were used. Without normalization, large-magnitude differences (maybe due to token length differences, etc.) could dominate the probe’s solution. However, given the small and uniform nature of the inputs, this likely wasn’t a big issue. The probe design itself is standard for linear probes.

Layer-wise Probes vs. Joint Analysis: They trained separate probes for each layer rather than, say, a single model that takes multi-layer inputs or a more complex classifier that could combine features from multiple layers. For the stated question (is it linearly encoded at some layer), the layer-by-layer approach is logical – it checks each layer independently for linear encodability. This helps identify if there is a specific layer where separation peaks (which they hypothesized might be later layers). The results ended up the same for all, but that was inconclusive due to issues discussed. The approach is sound for a probing study.

In summary, the activation analysis setup was conceptually appropriate for examining linear encodings, but certain choices (capturing only pre-generation states and mean-pooling across tokens) likely limited the probe’s ability to detect the faithfulness signal. These were acknowledged trade-offs. For a future, more thorough analysis, capturing the model’s state during its reasoning process and using more granular features would be advisable to truly answer whether CoT faithfulness is encoded in the activations in a linearly separable way.

Addressing the Primary Question: CoT Faithfulness Linear Encoding

The primary research question posed was: “Is CoT faithfulness linearly encoded in small reasoning models (1.5B), making real-time monitoring feasible with simple linear probes?”. How well does the research answer this?

Evidence from the Study: The study attempted to answer this by training linear classifiers to distinguish faithful vs unfaithful reasoning cases from internal activations. The outcome – a roughly 66-67% accurate probe (which is basically no better than chance/majority) – suggests that if faithfulness is encoded in the model’s activations, it is not in an easily accessible linear form (at least not in the pre-generation activations of this small model). In other words, the results mostly indicate that a simple linear probe is not sufficient to reliably identify faithful reasoning in this context. This directly speaks to the feasibility of real-time monitoring with linear probes: with only ~66% accuracy on a simple task, the approach would be unreliable in practice. The author interprets this as a “modest linear signal” existing, but as argued, the signal is extremely weak. The safer interpretation is that CoT faithfulness is not strongly linearly encoded in the model’s activations, at least not in a straightforward way detectable by this method. It might be encoded in a more complex (non-linear) manner, or require looking at activations during the chain-of-thought generation itself.

Late-Layer Encoding Hypothesis: There was a hint that by layer 24, the representations might separate faithful vs unfaithful cases more (the separation metric of 0.862). If this is taken at face value, it implies some encoding of faithfulness does emerge by the end of input processing. This would align with the idea that the model computes the necessary logic by the final layers, so it “knows” internally whether it will answer correctly or not, even if a linear readout struggled to extract that reliably. However, given the issues, this is speculative. If true, it suggests the answer to the question might be “yes, but only in the final layers and still not as a linearly trivial feature.”

Real-Time Monitoring Feasibility: The motivation was AI safety – if faithfulness could be detected by a simple probe, one could monitor a model’s reasoning in deployment. Based on this study, the answer leans negative: with only ~66% accuracy in a best case on trivial tasks, a linear probe is not a dependable monitor. The research thus provides an answer: CoT faithfulness is likely not robustly linearly encoded in small models, meaning more sophisticated methods (non-linear probes or other interpretability techniques) would be needed for real-time detection. This conclusion is actually reflected in the report’s interpretation that non-linear methods or multi-layer signals may be required.

In summary, the primary question was tackled head-on, but the results suggest a mostly negative or at best weak positive answer. The research addresses the question by providing a case study; however, due to its limitations, it doesn’t definitively settle the matter. It does indicate that if a linear encoding exists, it is faint. To truly confirm or refute the linear encoding of faithfulness, a more comprehensive study (with more data and refined methods) would be needed. Still, this work sets the stage by identifying the challenges involved.

Limitations and Caveats

The author has been upfront about many limitations, and this review concurs with their significance:

Small Data & Generalization: The tiny sample size (50 question pairs, 44 used in training, 9 in test) is a major limitation. It means any quantitative finding could be a fluke. Moreover, the task was a very specific one (simple numeric comparisons), so results may not generalize to other reasoning types (e.g. commonsense, multi-hop reasoning). The study also only looked at one model of one size, so we can’t generalize to other models or scales. This is acceptable for an initial exploration but must be kept in mind: we learned about this particular model on this particular task – nothing more broadly definitive.

Label Noise from Non-Determinism: As discussed, using non-deterministic generation for labeling introduced potential label noise. The same prompt could yield a different chain-of-thought and answer if run again, which undermines the label’s reliability. This limitation means the probe was trying to learn a noisy target, likely hurting performance. In future, using deterministic generation (temperature 0) or at least fixing the random seed for reproducibility would be important to ensure consistent labels. Alternatively, capturing activations during the actual sampled generation (instead of a separate deterministic pass) would align the features with the exact outcome.

Timing Mismatch of Activation Capture: The probe features were taken before the model actually generated its reasoning/answer, which might miss where faithfulness truly “happens” in the computation. This mismatch is a notable caveat – the probe wasn’t looking at the model while it was reasoning, only before it started producing the answer. It’s possible that faithfulness signals only fully form during the autoregressive generation. So this limitation suggests the study might not have been looking in the right place/time for a strong signal. The author frames it as the probe answering “Can input representations predict faithful generation?” which is a slightly different question than “Does the model’s full reasoning process linearly encode faithfulness?”. Future work should try to capture activations at the moment of answer generation or at multiple points in the reasoning chain.

Mean-Pooling and Loss of Information: Compressing each sequence’s activations to a single vector by averaging is a methodological limitation. Important sequence dynamics or specific token signals could be lost. The results might improve if the probe had access to more fine-grained information (for instance, the hidden state of the final token or an attention-weighted pool focusing on key parts of the prompt). The author suggests exploring token-level analysis or attention-based features in future work, which is a sound recommendation.

Class Imbalance & Evaluation Metric: The imbalance (around 2:1) between faithful and unfaithful cases means accuracy is a less informative metric; AUC was reported, but those values were also inconclusive. The limitation here is minor, but for better statistical insight, precision/recall or balanced accuracy could be considered, especially if expanding to larger data.

Uniform Probe Accuracy Across Layers: The fact that all layers’ probes yielded the same test accuracy is a red flag. The author speculates it could be due to the probe defaulting to majority class or simply the test set being too small to differentiate. This is more a result than a limitation per se, but it limits any conclusion about which layer is best. In effect, we learned nothing from the probe about layer differences because of this uniform outcome (besides the unreliable visualization suggesting the last layer might hold promise). It highlights again that the experiment’s resolution was too low to detect the subtle changes it aimed to measure.

The author rightly states that these limitations “don’t invalidate the work – they add noise and should be addressed in future iterations”. In agreement, none of these issues suggests the idea is fundamentally misguided, but they do mean the findings are preliminary. For instance, the high faithfulness rate observed could be partly due to the simplicity of the task; the low probe accuracy could be due to label noise and small sample. Addressing these would strengthen confidence in any results.

Suggestions for Methodological Improvements

To build on this project and address its shortcomings, several key improvements are recommended:

Increase Sample Size: Expanding the dataset to hundreds of question pairs (e.g. 200-500 pairs) would greatly improve statistical power. This would make probe training more reliable and allow for a proper held-out test that isn’t just single-digit examples. A larger sample would also enable splitting data for validation or trying cross-validation. The patterns observed (or lack thereof) would be far more trustworthy with more data.

Deterministic or Aligned Generation: Use deterministic generation (temperature 0) when labeling faithfulness to remove randomness from the equation. Alternatively, if some randomness is desired to explore model behavior, ensure that the activations captured correspond exactly to the generation that produced the label (for example, by recording the model’s states during the sampled chain-of-thought). This alignment would eliminate the label-feature mismatch and reduce noise in probe training.

Capture Activations During Reasoning: Instead of only caching the residual stream after the input, instrument the model to capture hidden states at critical points during the chain-of-thought and at the moment just before the answer is produced. For example, one could capture the activations at the final token of the chain-of-thought or at each step of the generated reasoning. This would directly test if faithful vs unfaithful reasoning trajectories diverge in activations. It could be done by generating with the model and using hooks in a framework like TransformerLens to grab states at each generation step.

Finer-Grained Feature Representation: Avoid blunt mean-pooling. Investigate token-level features or structured representations. Possibilities include: using the hidden state of a special “[CLS]” token if the model had one, concatenating the last-layer hidden states of the last few tokens of the question (which might capture the comparison result), or even using the entire sequence of activations with a more complex probe (like an RNN or attention mechanism that is still “small”). Even if one wants to keep it linear, one could try taking the difference between certain token representations (e.g., representation of the two numbers in the question) as features. The key is to preserve the information that might signal whether the model has correctly understood the comparison.

Use Non-Linear Probes or Methods: Since linear separability seems weak, explore non-linear classifiers or unsupervised methods to detect faithfulness. The author mentions sparse autoencoders or analyzing attention patterns as ideas. A simple step up would be a small multi-layer perceptron or even a kernel method to see if a slightly more complex decision boundary can improve accuracy. This, however, moves away from the strict “linearly encoded” question – but it helps answer whether the information is there at all (just in a non-linear form). Additionally, analyzing attention heads (e.g., are there attention heads that behave differently when reasoning is faithful?) could provide interpretable cues beyond what a linear probe can give.

Regularization and Calibration: If linear probes are used, apply appropriate regularization (L2, early stopping with validation) especially given the high dimensionality and low sample regime. Also ensure to include a bias term unless there’s a specific reason not to. Track not just accuracy but also other metrics like F1-score given class imbalance. Using techniques like bootstrapping the test evaluation or running multiple random splits would also yield a confidence estimate on performance.

Expand to Different Tasks and Models: To strengthen the conclusions, test the approach on more diverse reasoning tasks (e.g. logical puzzles, commonsense QA) and on larger models for comparison. This will show whether the patterns observed (like high faithfulness rate or late-layer encoding) hold generally or were quirks of the specific setup. For instance, if a 7B or 70B model is evaluated on the same numeric comparison task, do we actually see a difference in faithfulness rate or probe separability? That would directly answer the question of scaling effects on CoT faithfulness, which currently the study could only speculate on via literature.

Causal Experiments: The author suggests clever interventions like activation patching – e.g., feed a known faithful trajectory’s activations into an unfaithful run to see if it corrects it. This would test causally if certain activation differences cause faithfulness. While not necessary for just improving the probe, such experiments could deepen understanding of where in the model the faithfulness decision is happening. If, say, patching layer 24 from a faithful run into an unfaithful one forces the answer to become correct, that strongly indicates layer 24 encodes the deciding factor. These interventions can complement probing by pinpointing causation rather than just correlation.

Implementing these improvements would address most of the fatal flaws and limitations identified. In particular, increasing data and aligning activations with generation would likely yield a clearer picture of linear vs non-linear encodings of faithfulness. Non-linear analyses and interventions would help confirm if the small linear signal observed is the tip of a deeper representational iceberg or just noise.

Conclusion

Overall, this research project is a valuable initial foray into mechanistically evaluating chain-of-thought faithfulness in a small model. It demonstrates a workable question-flipping evaluation and sets up an infrastructure for probing internal activations. The core finding is that this 1.5B model can reason fairly consistently on simple tasks (72% faithfulness), but detecting that reasoning quality via a simple linear probe is challenging and currently not reliable (≈66% accuracy, essentially baseline). There were no evident fatal errors in execution, but the experimental scope was so limited that results remain extremely tentative. The conclusions drawn are directionally reasonable (small models might encode faithfulness weakly; any such encoding seems to happen late in the network), but they are not strongly substantiated due to statistical weakness.

Crucially, the study highlights several issues – label noise from stochastic generation, mismatch of activation timing, tiny sample size – that likely obscured the true signal. Addressing these will be necessary to get a clear answer to whether CoT faithfulness is encoded in hidden states in a linearly decodable way. As it stands, the answer leans toward “not in an easily linear decodable form,” but with the caveat that this conclusion could be influenced by the above limitations.

In terms of the primary question of real-time monitoring feasibility: a 66.7% accurate linear probe is not sufficient for reliable monitoring, implying that more advanced techniques or more information (like using the chain-of-thought content or non-linear models) would be required for practical detection of reasoning fidelity. The late-layer emergence hint suggests the model’s internal logic about the answer crystallizes in the final layers, which is intuitively plausible. Future work that zeroes in on those final-layer signals (with better data and methods) could confirm if there is a latent “faithfulness neuron set” or combination that could be monitored.

In conclusion, Denis Lim’s project provides an insightful starting point and a solid experimental framework, but the evidence is too limited to make strong claims yet. The study successfully identifies the challenges in this line of inquiry and offers a foundation for further research. By scaling up and refining the methodology as suggested, we can better determine if chain-of-thought faithfulness is something we can read out from a model’s activations – linearly or otherwise – and thereby enhance our ability to trust and verify AI reasoning processes.