# Layer Progression Animations

## Overview

These animations visualize how activation representations evolve across layers (6 → 12 → 18 → 24), showing the trajectory of faithful vs unfaithful responses through the model's internal processing.

**Generated by:** `animate_layer_progression.py`

---

## Animation Types

### 1. **layer_progression_probe.gif**

**What it shows:** Projection onto the learned probe direction (the "faithfulness axis")

- **X-axis:** Projection onto probe direction (higher = more faithful according to the probe)
- **Y-axis:** Random jitter for visibility (no semantic meaning)
- **Interpretation:** This is the most **interpretable** visualization because the probe explicitly learned to separate faithful from unfaithful responses
- **What to look for:**
  - Do faithful (blue) and unfaithful (red) points separate over layers?
  - At which layer does the separation become maximal?
  - Are there outliers (unfaithful points that look faithful, or vice versa)?

**Key insight:** Shows how the model's internal "faithfulness score" evolves.

---

### 2. **layer_progression_pca_per_layer.gif**

**What it shows:** 2D PCA projection computed independently for each layer

- **X-axis:** First principal component (PC1) - direction of maximum variance
- **Y-axis:** Second principal component (PC2) - direction of 2nd-most variance
- **Interpretation:** Shows the "natural" structure of activations at each layer
- **What to look for:**
  - How does the overall geometry change?
  - Do the clusters get tighter or more spread out?
  - Is there clear separation at any layer?

**Caveat:** Each layer has its own PCA basis, so the coordinate system changes across frames. This makes cross-layer comparison harder but maximizes separation at each individual layer.

---

### 3. **layer_progression_pca_global.gif**

**What it shows:** 2D PCA projection with a **shared basis across all layers**

- **X-axis:** Global PC1 (fitted on all layers concatenated)
- **Y-axis:** Global PC2
- **Interpretation:** Uses a consistent coordinate system, making cross-layer trajectories directly comparable
- **What to look for:**
  - Do points move in a consistent direction as layers increase?
  - Is there a clear "flow" from early to late layers?
  - Do faithful and unfaithful responses follow different paths?

**Key advantage:** Direct comparison across layers (same axes throughout animation).

---

### 4. **layer_progression_combined.gif** ⭐ RECOMMENDED

**What it shows:** Combination of probe direction (X) and global PCA (Y)

- **X-axis:** Probe direction projection (interpretable "faithfulness score")
- **Y-axis:** First global principal component (maximum variance)
- **Interpretation:** Best of both worlds - interpretable X-axis + variance-capturing Y-axis
- **What to look for:**
  - Horizontal separation = faithfulness difference (this is what we care about!)
  - Vertical spread = other sources of variation
  - Does the horizontal separation increase in middle layers? (This would suggest layer 12-18 is where faithfulness is computed)

**This is likely the most useful visualization for understanding your model!**

---

## Key Research Questions

Use these animations to investigate:

1. **When does separation emerge?**
   - Are faithful/unfaithful responses already distinct at layer 6?
   - Or does the separation gradually increase?
   - Is there a "critical layer" where they suddenly diverge?

2. **What is the trajectory?**
   - Do unfaithful responses start "faithful-like" and then diverge?
   - Or are they always distinct?

3. **Which layer is best for detection?**
   - At which layer is the separation maximal?
   - Does this match the probe's best layer (layer 12)?

4. **Are there outliers?**
   - Unfaithful responses that project as "faithful" (false negatives)
   - Faithful responses that project as "unfaithful" (false positives)
   - Do these outliers follow unusual trajectories?

---

## Statistics Displayed

Each animation shows real-time statistics:

- **Probe animations:**
  - Mean (μ) and standard deviation (σ) of projections
  - Separation: absolute distance between faithful and unfaithful means
  
- **PCA animations:**
  - Separation: Euclidean distance between centroids
  - Within-group spread: how tight each cluster is

---

## Viewing the Animations

**In macOS:**
```bash
open results/activation_visualizations/layer_progression_combined.gif
```

**In browser:**
Simply drag the `.gif` file into your browser.

**Frame-by-frame:**
Most image viewers (e.g., Preview on Mac) let you pause and step through frames manually.

---

## Customization

### Change Animation Speed

```bash
# Slower (1 frame per second)
python animate_layer_progression.py --fps 1

# Faster (4 frames per second)
python animate_layer_progression.py --fps 4
```

### Generate Only One Type

```bash
# Only probe direction
python animate_layer_progression.py --mode probe

# Only global PCA
python animate_layer_progression.py --mode global_pca

# Only combined
python animate_layer_progression.py --mode combined
```

### Use Different Layers

```bash
# Custom layer selection (if you have activations for them)
python animate_layer_progression.py --layers 3 6 9 12 15 18 21 24
```

---

## Technical Details

### Dimensionality Reduction Methods

1. **Probe Direction Projection:**
   - Takes the learned linear probe weight vector `w` (shape: `[d_model]`)
   - Projects each activation: `projection = activation @ w` (scalar)
   - This is a 1D projection, so we add random jitter for 2D visualization

2. **PCA (Per-Layer):**
   - Fits PCA separately on each layer's data
   - Maximizes variance at each layer independently
   - Each layer has its own basis vectors

3. **PCA (Global):**
   - Concatenates activations from ALL layers
   - Fits one PCA on the combined dataset
   - All layers share the same basis vectors

### Data Used

- **Faithful responses:** 30 samples (from training data)
- **Unfaithful responses:** 14 samples (from training data)
- **Layers:** 6, 12, 18, 24 (from Phase 3 training)
- **Activation source:** `data/activations/layer_{N}_activations.pt`
- **Probe source:** `results/probe_results/all_probe_results.pt`

---

## Interpretation Guide

### Good Signs

✅ **Separation increases across layers** → Model is building up faithfulness representation

✅ **Maximal separation at layer 12-18** → Matches probe performance

✅ **Tight clusters** → Consistent behavior within each group

✅ **Smooth trajectories** → Gradual transformation

### Warning Signs

⚠️ **No separation at any layer** → Faithfulness may not be linearly represented

⚠️ **Separation at layer 6 already** → May just be detecting surface patterns

⚠️ **Erratic movement** → Unstable or noisy representations

⚠️ **Large overlap** → Linear probe may not be sufficient

---

## Next Steps

1. **Watch the animations** and form hypotheses about when/how faithfulness emerges

2. **Identify outliers** - which specific samples behave unusually?

3. **Test on new data** - do the patterns hold on the test set?
   ```bash
   python animate_layer_progression.py --activations-dir data/test_activations
   ```

4. **Extract more layers** - get finer-grained view of the progression

5. **Track individual samples** - create line plots showing one sample's trajectory

---

## Questions?

This visualization technique is based on standard practices in mechanistic interpretability. The animation approach is particularly useful for understanding **how** representations change, not just **what** they are at each layer.

For more details, see:
- `animate_layer_progression.py` - the implementation
- `technical_specification.md` - Phase 3 methodology
- `view_activations.py` - static visualization tools

